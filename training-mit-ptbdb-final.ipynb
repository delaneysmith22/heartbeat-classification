{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "mit_train_data = pd.read_csv('/kaggle/input/heartbeat/mitbih_train.csv',header=None)\n",
    "mit_test_data = pd.read_csv('/kaggle/input/heartbeat/mitbih_test.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training point, number of time points\n",
    "np.shape(mit_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes: ['N': 0, 'S': 1, 'V': 2, 'F': 3, 'Q': 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training points in each class\n",
    "    # Column 187 = class labels\n",
    "trainN = mit_train_data[mit_train_data[187]==0] \n",
    "trainS = mit_train_data[mit_train_data[187]==1] \n",
    "trainV = mit_train_data[mit_train_data[187]==2] \n",
    "trainF = mit_train_data[mit_train_data[187]==3] \n",
    "trainQ = mit_train_data[mit_train_data[187]==4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the testing points in each class\n",
    "    # Column 187 = class labels\n",
    "testN = mit_test_data[mit_test_data[187]==0] \n",
    "testS = mit_test_data[mit_test_data[187]==1] \n",
    "testV = mit_test_data[mit_test_data[187]==2] \n",
    "testF = mit_test_data[mit_test_data[187]==3] \n",
    "testQ = mit_test_data[mit_test_data[187]==4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of training points in each class\n",
    "n_trainN = trainN.shape[0]\n",
    "n_trainS = trainS.shape[0]\n",
    "n_trainV = trainV.shape[0]\n",
    "n_trainF = trainF.shape[0]\n",
    "n_trainQ = trainQ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of test points in each class\n",
    "n_testN = testN.shape[0]\n",
    "n_testS = testS.shape[0]\n",
    "n_testV = testV.shape[0]\n",
    "n_testF = testF.shape[0]\n",
    "n_testQ = testQ.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot data set (recreate Figure 4). We plot the first 50 beat signals all together as well as the averaged beat signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put data in propoer form for plotting\n",
    "trainN_plot = trainN.drop(columns = [187]).values\n",
    "trainS_plot = trainS.drop(columns = [187]).values\n",
    "trainV_plot = trainV.drop(columns = [187]).values\n",
    "trainF_plot = trainF.drop(columns = [187]).values\n",
    "trainQ_plot = trainQ.drop(columns = [187]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average of first 50 heartbeats in each class\n",
    "trainN_avg = np.mean(trainN_plot[:50],axis=0)\n",
    "trainS_avg = np.mean(trainS_plot[:50],axis=0)\n",
    "trainV_avg = np.mean(trainV_plot[:50],axis=0)\n",
    "trainF_avg = np.mean(trainF_plot[:50],axis=0)\n",
    "trainQ_avg = np.mean(trainQ_plot[:50],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate Figure 4\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(3,2,1)\n",
    "for i in range(50):\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.plot(trainN_plot[i],'tab:red',alpha=0.1)\n",
    "plt.plot(trainN_avg,'k',linewidth=2)\n",
    "plt.title('N')\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Amplitude (mV)')\n",
    "\n",
    "plt.subplot(3,2,2)\n",
    "for i in range(50):\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.plot(trainS_plot[i],'tab:blue',alpha=0.1)\n",
    "plt.plot(trainS_avg,'k',linewidth=2)\n",
    "plt.title('S')\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Amplitude (mV)')\n",
    "\n",
    "plt.subplot(3,2,3)\n",
    "for i in range(50):\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.plot(trainV_plot[i],'tab:green',alpha=0.1)\n",
    "plt.plot(trainV_avg,'k',linewidth=2)\n",
    "plt.title('V')\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Amplitude (mV)')\n",
    "\n",
    "plt.subplot(3,2,4)\n",
    "for i in range(50):\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.plot(trainF_plot[i],'tab:purple',alpha=0.1)\n",
    "plt.plot(trainF_avg,'k',linewidth=2)\n",
    "plt.title('F')\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Amplitude (mV)')\n",
    "\n",
    "plt.subplot(3,2,5)\n",
    "for i in range(50):\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.plot(trainQ_plot[i],'tab:orange',alpha=0.1)\n",
    "plt.plot(trainQ_avg,'k',linewidth=2)\n",
    "plt.title('Q')\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Amplitude (mV)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(train):\n",
    "    X_train = np.asarray(train.drop(columns=[187]))\n",
    "    y_train = np.asarray(train[187])\n",
    "    indexes = np.arange(int(X_train.shape[0]))\n",
    "    indexes = np.random.RandomState(seed=42).permutation(indexes)  # shuffle data to randomly select\n",
    "    X_train_shuffled = X_train[indexes]\n",
    "    y_train_shuffled = y_train[indexes]\n",
    "    y_train_shuffled = y_train_shuffled.astype(int)\n",
    "    y_train_shuffled_cat = to_categorical(y_train_shuffled)\n",
    "    return X_train_shuffled,y_train_shuffled,y_train_shuffled_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, labels, y_train = shuffle(mit_train_data)\n",
    "X_train_new = X_train.reshape(87554,187,1)\n",
    "\n",
    "X_test, test_labels, y_test = shuffle(mit_test_data)\n",
    "X_test_new = X_test.reshape(X_test.shape[0],187,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define focal loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define focal loss function as per \n",
    "    https://www.dlology.com/blog/\n",
    "    multi-class-classification-with-focal-loss-for-imbalanced-datasets/\"\"\" \n",
    "def focal_loss_fnc(gamma,alpha):\n",
    "    \n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "    \n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"Focal loss for multi-classification\n",
    "        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
    "        Notice: y_pred is probability after softmax\n",
    "        gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper\n",
    "        d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)\n",
    "        Focal Loss for Dense Object Detection\n",
    "        https://arxiv.org/abs/1708.02002\n",
    "\n",
    "        Arguments:\n",
    "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
    "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
    "\n",
    "        Keyword Arguments:\n",
    "            gamma {float} -- (default: {2.0})\n",
    "            alpha {float} -- (default: {4.0})\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9\n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "\n",
    "        model_out = tf.add(y_pred, epsilon)\n",
    "        ce = tf.multiply(y_true, -tf.math.log(model_out))\n",
    "        weight = tf.multiply(y_true, tf.pow(tf.subtract(1., model_out), gamma))\n",
    "        fl = tf.multiply(alpha, tf.multiply(weight, ce))\n",
    "        reduced_fl = tf.reduce_max(fl, axis=1)\n",
    "        return tf.reduce_mean(reduced_fl)\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load trained models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../input/models/Models/model_wout_focal_loss\"\n",
    "model_wout_fl = load_model(filename,\n",
    "                           custom_objects=None,\n",
    "                           compile=True)\n",
    "history_wout_fl = pd.read_csv(\"../input/models/Models/history_wout_focal_loss.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../input/models/Models/model_focal_loss\"\n",
    "model_fl = load_model(filename,\n",
    "                           custom_objects={'focal_loss_fixed': focal_loss_fnc(gamma=2,alpha=0.25)},\n",
    "                           compile=True)\n",
    "history_fl = pd.read_csv(\"../input/models/Models/history_focal_loss.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate models performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_p_wout_fl = model_wout_fl.predict(X_test_new)\n",
    "y_test_wout_fl = np.argmax(pred_p_wout_fl,axis=1)\n",
    "pred_p_fl = model_fl.predict(X_test_new)\n",
    "y_test_fl = np.argmax(pred_p_fl,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_acc(model, X_test, y_test):\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    test_loss = score[0]\n",
    "    test_acc = score[1]\n",
    "    print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_wout_fl, acc_wout_fl = test_loss_acc(model_wout_fl,X_test_new,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fl, acc_fl = test_loss_acc(model_fl,X_test_new,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot curves (recreate figure 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_wout_fl = np.asarray(history_wout_fl['loss'])\n",
    "train_acc_wout_fl = np.asarray(history_wout_fl['accuracy'])\n",
    "train_loss_fl = np.asarray(history_fl['loss'])\n",
    "train_acc_fl = np.asarray(history_fl['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.linspace(1,100,100)\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs,train_loss_wout_fl,'b',linewidth=2,label='Model 1 training loss (0.06951)')\n",
    "plt.plot(epochs,train_loss_fl,'r',linewidth=2,label='Model 1 traning loss (0.00612)')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs,train_acc_wout_fl,'b',linewidth=2,label='Model 1 training accuracy (0.98314)')\n",
    "plt.plot(epochs,train_acc_fl,'r',linewidth=2,label='Model 1 training accuracy (0.98360)')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('training_loss_acc.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: drop in loss/increase in accuracy at 80 epochs is due to change in learning rate (0.001 to 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Metrics (Precision, Recall, F1-score)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report: Model 1 (without focal loss)\")\n",
    "print(classification_report(test_labels, y_test_wout_fl,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report: Model 2 (with focal loss)\")\n",
    "print(classification_report(test_labels, y_test_fl,digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Paper has a range of values (must have trained multiple times - might want to train a second model to see if things stay relatively the same or change drastically)\n",
    "* In the paper, most values increase or stay relatively consistent when focal loss is added\n",
    "* Here, without focal loss class 1 precision was much higher than expected, while recall was much lower than expected (low number of false negatives, but also positives)\n",
    "* With focal loss added, precision significantly decreased while recall significantly increased i.e. focal loss identified more true positives but also more false positives\n",
    "* To help visualize this, a row and a column were added to confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUC**\n",
    "\n",
    "* Can use the one vs all classification needed to create Fig 8a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_wout_fl = roc_auc_score(test_labels,pred_p_wout_fl,average='weighted',multi_class='ovr')\n",
    "auc_fl = roc_auc_score(test_labels,pred_p_fl,average='weighted',multi_class='ovr')\n",
    "print('AUC Model 1 (without fl): %.4f' % auc_wout_fl)\n",
    "print('AUC Model 2 (with fl): %.4f' % auc_fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_wout_fl = confusion_matrix(test_labels, y_test_wout_fl)\n",
    "print(\"Confusion matrix: Model 1 (without focal loss)\")\n",
    "print(cm_wout_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_fl = confusion_matrix(test_labels, y_test_fl)\n",
    "print(\"Confusion matrix: Model 2 (with focal loss)\")\n",
    "print(cm_fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict on unshuffled data (Figure 8b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ns = np.asarray(mit_test_data.drop(columns=[187]))\n",
    "X_test_ns_new = X_test_ns.reshape(X_test_ns.shape[0],187,1)\n",
    "y_test_ns = np.asarray(mit_test_data[187])\n",
    "y_test_ns_cat = to_categorical(y_test_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_p_fl_ns = model_fl.predict(X_test_ns_new)\n",
    "y_test_fl_ns = np.argmax(pred_p_fl_ns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified points\n",
    "diff = y_test_fl_ns - y_test_ns\n",
    "mask = diff != 0\n",
    "miss = np.copy(y_test_fl_ns)\n",
    "miss[mask] = y_test_fl_ns[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(np.linspace(1,len(y_test_ns),len(y_test_ns)),y_test_ns,s=70,c='b',alpha=0.5)\n",
    "plt.scatter(np.linspace(1,len(y_test_ns),len(y_test_ns)),y_test_fl_ns,c='r',alpha=0.5)\n",
    "for i in range(len(y_test_ns)):\n",
    "    if y_test_ns[i] != y_test_fl_ns[i]:\n",
    "        plt.scatter(i,y_test_fl_ns[i],s=5,c='k')\n",
    "plt.legend([\"True\",\"Predicted\",\"Missclassified\"],loc='upper center',facecolor='white',framealpha=1)\n",
    "plt.xlabel('Samples')\n",
    "plt.yticks([0,1,2,3,4])\n",
    "plt.ylabel('Arrythmia Classes')\n",
    "plt.savefig(\"test_classification.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding FP and FN to Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_pos(index,cm):\n",
    "    column = cm[:,index]\n",
    "    FP = sum(column)-column[index]\n",
    "    return FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_neg(index,cm):\n",
    "    row = cm[index,:]\n",
    "    FN = sum(row)-row[index]\n",
    "    return FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False pos = sum of the column - diag (row at bottom)\n",
    "def false_pos_vec(cm):\n",
    "    FP = np.empty(5)\n",
    "    for i in range(5):\n",
    "        FPi = false_pos(i,cm)\n",
    "        FP[i] = FPi\n",
    "    return FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False neg = sum of the row - diag (column at end)\n",
    "def false_neg_vec(cm):\n",
    "    FN = np.empty(5)\n",
    "    for i in range(5):\n",
    "        FNi = false_neg(i,cm)\n",
    "        FN[i] = FNi\n",
    "    return FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_cm(cm):\n",
    "    FP_vec = false_pos_vec(cm)\n",
    "    FN_vec = false_neg_vec(cm)\n",
    "    new_cm = pd.DataFrame(data=cm)\n",
    "    new_cm['FN'] = FN_vec\n",
    "    FP_df = pd.DataFrame([FP_vec],index=[\"FP\"])\n",
    "    new_cm = pd.concat([new_cm,FP_df,])\n",
    "    return new_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_wout_fl_df = create_new_cm(cm_wout_fl)\n",
    "cm_wout_fl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_fl_df = create_new_cm(cm_fl)\n",
    "cm_fl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Class 1: Number of TP (diagonal) increased, but so did the number of FP (why recall increased but precision decreased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curves (one vs rest classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New binary label function\n",
    "def bin_labels(index,labels):\n",
    "    new_labels = np.copy(labels)\n",
    "    new_labels[new_labels==index]=10\n",
    "    new_labels[new_labels!=10]=0\n",
    "    new_labels[new_labels==10]=1\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(index,data,shuffle=True):\n",
    "    X_train = np.asarray(data.drop(columns=[187]))\n",
    "    y_train = np.asarray(data[187])\n",
    "    y_train = bin_labels(index,y_train)\n",
    "    if shuffle:\n",
    "        indexes = np.arange(int(X_train.shape[0]))\n",
    "        indexes = np.random.RandomState(seed=42).permutation(indexes)  # shuffle data to randomly select\n",
    "        X_train_new = X_train[indexes]\n",
    "        y_train_new= y_train[indexes]\n",
    "    else:\n",
    "        X_train_new = X_train\n",
    "        y_train_new = y_train\n",
    "    y_train_new = y_train_new.astype(int)\n",
    "    y_train_new_cat = to_categorical(y_train_new)\n",
    "    return X_train_new,y_train_new,y_train_new_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using multi class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_labels0 = bin_labels(0,test_labels)\n",
    "bin_labels1 = bin_labels(1,test_labels)\n",
    "bin_labels2 = bin_labels(2,test_labels)\n",
    "bin_labels3 = bin_labels(3,test_labels)\n",
    "bin_labels4 = bin_labels(4,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr0, tpr0, threshold0 = metrics.roc_curve(bin_labels0,pred_p_fl[:,0])\n",
    "fpr1, tpr1, threshold1 = metrics.roc_curve(bin_labels1,pred_p_fl[:,1])\n",
    "fpr2, tpr2, threshold2 = metrics.roc_curve(bin_labels2,pred_p_fl[:,2])\n",
    "fpr3, tpr3, threshold3 = metrics.roc_curve(bin_labels3,pred_p_fl[:,3])\n",
    "fpr4, tpr4, threshold4 = metrics.roc_curve(bin_labels4,pred_p_fl[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc0 = roc_auc_score(bin_labels0,pred_p_fl[:,0])\n",
    "auc1 = roc_auc_score(bin_labels1,pred_p_fl[:,1])\n",
    "auc2 = roc_auc_score(bin_labels2,pred_p_fl[:,2])\n",
    "auc3 = roc_auc_score(bin_labels3,pred_p_fl[:,3])\n",
    "auc4 = roc_auc_score(bin_labels4,pred_p_fl[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,6))\n",
    "plt.plot(fpr0, tpr0,'m',label='ROC curve of class 0',linewidth=2)\n",
    "plt.plot(fpr1, tpr1,'c',label='ROC curve of class 1',linewidth=2)\n",
    "plt.plot(fpr2, tpr2,'g',label='ROC curve of class 2',linewidth=2)\n",
    "plt.plot(fpr3, tpr3,'b',label='ROC curve of class 3',linewidth=2)\n",
    "plt.plot(fpr4, tpr4,'k',label='ROC curve of class 4',linewidth=2)\n",
    "plt.plot([0,1],[0,1],'r',label='Random Guess',linestyle='--',linewidth=2)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0 vs rest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../input/model-0vr/model_0vR\"\n",
    "model_0vR = load_model(filename,\n",
    "                           custom_objects={'focal_loss_fixed': focal_loss_fnc(gamma=2,alpha=0.25)},\n",
    "                           compile=True)\n",
    "history_0vR = pd.read_csv(\"../input/model-0vr/model_0vR/history_0vR.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new test set in proper form\n",
    "X_test0, test_labels0, y_test0 = pre_process(0,mit_test_data,shuffle=False)\n",
    "X_test_new0 = X_test0.reshape(X_test0.shape[0],187,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict binary labels\n",
    "pred_p_0vR = model_0vR.predict(X_test_new0)\n",
    "y_test_0vR = np.argmax(pred_p_0vR,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure test labels match what you expect\n",
    "test_labels0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_0vR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve on 0vR model\n",
    "fpr0_new, tpr0_new, threshold0_new = metrics.roc_curve(test_labels0,pred_p_0vR[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC of 0vR model\n",
    "auc0_new = roc_auc_score(test_labels0,pred_p_0vR[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_auc0 = auc0_new - auc0\n",
    "comp_auc0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both ROC curves\n",
    "plt.plot(fpr0_new, tpr0_new,label='ROC curve of class 0')\n",
    "plt.plot(fpr0, tpr0,label='ROC curve of class 0 (method 2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 vs rest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../input/model-1vr/model_1vR\"\n",
    "model_1vR = load_model(filename,\n",
    "                           custom_objects={'focal_loss_fixed': focal_loss_fnc(gamma=2,alpha=0.25)},\n",
    "                           compile=True)\n",
    "history_1vR = pd.read_csv(\"../input/model-1vr/model_1vR/history_1vR.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new test set in proper form\n",
    "X_test1, test_labels1, y_test1 = pre_process(1,mit_test_data,shuffle=False)\n",
    "X_test_new1 = X_test1.reshape(X_test1.shape[0],187,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict binary labels\n",
    "pred_p_1vR = model_1vR.predict(X_test_new1)\n",
    "y_test_1vR = np.argmax(pred_p_1vR,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure test labels match what you expect\n",
    "test_labels1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_1vR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve on 0vR model\n",
    "fpr1_new, tpr1_new, threshold1_new = metrics.roc_curve(test_labels1,pred_p_1vR[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC of 1vR model\n",
    "auc1_new = roc_auc_score(test_labels1,pred_p_1vR[:,1])\n",
    "auc1_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_auc1 = auc1_new - auc1\n",
    "comp_auc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr1_new, tpr1_new,label='ROC curve of class 1')\n",
    "plt.plot(fpr1, tpr1,label='ROC curve of class 1 (method 2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Estimate our ROC curves using their technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_pos(index,cm):\n",
    "    TP = cm[index,index]\n",
    "    return TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_neg(index,cm):\n",
    "    diag = np.diagonal(cm)\n",
    "    TN = sum(diag) - diag[index]\n",
    "    return TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_R(index,cm):\n",
    "    TP = true_pos(index,cm)\n",
    "    FN = false_neg(index,cm)\n",
    "    R = TP/(TP + FN)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_SPE(index,cm):\n",
    "    FP = false_pos(index,cm)\n",
    "    TN = true_neg(index,cm)\n",
    "    SPE = TN/(TN+FP)\n",
    "    return SPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR0 = per_class_R(0,cm_fl)\n",
    "FPR0 = 1-per_class_SPE(0,cm_fl)\n",
    "\n",
    "TPR1 = per_class_R(1,cm_fl)\n",
    "FPR1 = 1-per_class_SPE(1,cm_fl)\n",
    "\n",
    "TPR2 = per_class_R(2,cm_fl)\n",
    "FPR2 = 1-per_class_SPE(2,cm_fl)\n",
    "\n",
    "TPR3 = per_class_R(3,cm_fl)\n",
    "FPR3 = 1-per_class_SPE(3,cm_fl)\n",
    "\n",
    "TPR4 = per_class_R(4,cm_fl)\n",
    "FPR4 = 1-per_class_SPE(4,cm_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_FPR = (1/5)*(FPR0+FPR1+FPR2+FPR3+FPR4)\n",
    "avg_TPR = (1/5)*(TPR0+TPR1+TPR2+TPR3+TPR4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([0,avg_FPR,1],[0,avg_TPR,1],linestyle='--',linewidth=2,label='macro-average ROC')\n",
    "plt.plot([0,FPR0,1],[0,TPR0,1],'m',label='ROC of class 0',linewidth=2)\n",
    "plt.plot([0,FPR1,1],[0,TPR1,1],'c',label='ROC of class 1',linewidth=2)\n",
    "plt.plot([0,FPR2,1],[0,TPR2,1],'g',label='ROC of class 2',linewidth=2)\n",
    "plt.plot([0,FPR3,1],[0,TPR3,1],'b',label='ROC of class 3',linewidth=2)\n",
    "plt.plot([0,FPR4,1],[0,TPR4,1],'k',label='ROC of class 4',linewidth=2)\n",
    "plt.plot([0,1],[0,1],'r',linestyle='--',label='Random Guess',linewidth=2)\n",
    "plt.xlim([0,1])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,10))\n",
    "\n",
    "plt.subplot(3,2,1)\n",
    "plt.plot(fpr0, tpr0,'tab:red',linewidth=2)\n",
    "plt.plot([0,FPR0,1],[0,TPR0,1],'tab:red',linewidth=2,linestyle='--',label='Paper')\n",
    "plt.title('N(0)')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.subplot(3,2,2)\n",
    "plt.plot(fpr1, tpr1,'tab:blue',linewidth=2)\n",
    "plt.plot([0,FPR1,1],[0,TPR1,1],'tab:blue',linewidth=2,linestyle='--',label='Paper')\n",
    "plt.title('S(1)')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.subplot(3,2,3)\n",
    "plt.plot(fpr2, tpr2,'tab:green',linewidth=2)\n",
    "plt.plot([0,FPR2,1],[0,TPR2,1],'tab:green',linewidth=2,linestyle='--',label='Paper')\n",
    "plt.title('V(2)')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.subplot(3,2,4)\n",
    "plt.plot(fpr3, tpr3,'tab:purple',linewidth=2)\n",
    "plt.plot([0,FPR3,1],[0,TPR3,1],'tab:purple',linewidth=2,linestyle='--',label='Paper')\n",
    "plt.title('F(3)')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.subplot(3,2,5)\n",
    "plt.plot(fpr4, tpr4,'tab:orange',linewidth=2)\n",
    "plt.plot([0,FPR4,1],[0,TPR4,1],'tab:orange',linewidth=2,linestyle='--',label='Paper')\n",
    "plt.title('Q(4)')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ROC_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTBD Model/Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "PTBD_train_data = pd.read_csv('/kaggle/input/ptbd-data/PTBD_train.csv')\n",
    "PTBD_test_data = pd.read_csv('/kaggle/input/ptbd-data/PTBD_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(train):\n",
    "    X_train = np.asarray(train.drop(columns=['187']))\n",
    "    y_train = np.asarray(train['187'])\n",
    "    y_train_cat = to_categorical(y_train)\n",
    "    return X_train,y_train,y_train_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ptbd, labels_ptbd, y_train_ptbd = pre_process(PTBD_train_data)\n",
    "X_train_new_ptbd = X_train_ptbd.reshape(X_train_ptbd.shape[0],187,1)\n",
    "\n",
    "X_test_ptbd, test_labels_ptbd, y_test_ptbd = pre_process(PTBD_test_data)\n",
    "X_test_new_ptbd = X_test_ptbd.reshape(X_test_ptbd.shape[0],187,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_ptbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels_ptbd, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "filename = \"../input/model-ptbd/model_PTBD\"\n",
    "model_ptbd = load_model(filename,\n",
    "                           custom_objects={'focal_loss_fixed': focal_loss_fnc(gamma=2,alpha=0.25)},\n",
    "                           compile=True)\n",
    "history_ptbd = pd.read_csv(\"../input/model-ptbd/model_PTBD/history_PTBD.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "pred_p_ptbd = model_ptbd.predict(X_test_new_ptbd)\n",
    "y_test_ptbd = np.argmax(pred_p_ptbd,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report: PTBD Data-set\")\n",
    "print(classification_report(test_labels_ptbd, y_test_ptbd,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_ptbd = roc_auc_score(test_labels_ptbd,pred_p_ptbd[:,1])\n",
    "auc_ptbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_ptbd, tpr_ptbd,threshold_ptbd = metrics.roc_curve(test_labels_ptbd,pred_p_ptbd[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_ptbd = confusion_matrix(test_labels_ptbd, y_test_ptbd)\n",
    "print(\"Confusion matrix: PTB Data-set\")\n",
    "print(cm_ptbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR_ptbd = per_class_R(1,cm_ptbd)\n",
    "FPR_ptbd = 1-per_class_SPE(1,cm_ptbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr_ptbd,tpr_ptbd)\n",
    "plt.plot([0,FPR_ptbd,1],[0,TPR_ptbd,1],'--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('PTB Data-set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PTB Model without focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "filename = \"../input/model-ptbd-wout-fl/model_ptbd_wout_focal_loss\"\n",
    "model_ptbd_wout_fl = load_model(filename,\n",
    "                           custom_objects=None,\n",
    "                           compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "pred_p_ptbd_wout_fl = model_ptbd_wout_fl.predict(X_test_new_ptbd)\n",
    "y_test_ptbd_wout_fl = np.argmax(pred_p_ptbd_wout_fl,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report: PTBD Data-set\")\n",
    "print(classification_report(test_labels_ptbd, y_test_ptbd_wout_fl,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_ptbd_wout_fl = metrics.roc_auc_score(test_labels_ptbd,pred_p_ptbd_wout_fl[:,1])\n",
    "auc_ptbd_wout_fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_ptbd_wout_fl, tpr_ptbd_wout_fl,threshold_ptbd_wout_fl = metrics.roc_curve(test_labels_ptbd,pred_p_ptbd_wout_fl[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_ptbd_wout_fl = confusion_matrix(test_labels_ptbd, y_test_ptbd_wout_fl)\n",
    "print(\"Confusion matrix: PTB Data-set\")\n",
    "print(cm_ptbd_wout_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr_ptbd_wout_fl,tpr_ptbd_wout_fl)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('PTB Data-set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
